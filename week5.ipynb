{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNsVrQQUhr4sCJ0oVi8dcly",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/JaynthReddy91/DL/blob/main/week5.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FOcqU_Yk9Qcw",
        "outputId": "6ab13b1e-54c4-4216-8245-3a29de8eb501"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "\u001b[1m11490434/11490434\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 0us/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.8266 - loss: 1.2568 - val_accuracy: 0.9179 - val_loss: 0.5502\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9147 - loss: 0.5460 - val_accuracy: 0.9352 - val_loss: 0.4553\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9269 - loss: 0.4718 - val_accuracy: 0.9307 - val_loss: 0.4355\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9342 - loss: 0.4323 - val_accuracy: 0.9423 - val_loss: 0.4002\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9384 - loss: 0.4006 - val_accuracy: 0.9419 - val_loss: 0.3807\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.9391 - loss: 0.3891 - val_accuracy: 0.9485 - val_loss: 0.3590\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9421 - loss: 0.3675 - val_accuracy: 0.9398 - val_loss: 0.3587\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9456 - loss: 0.3532 - val_accuracy: 0.9534 - val_loss: 0.3215\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.9462 - loss: 0.3406 - val_accuracy: 0.9503 - val_loss: 0.3256\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.9466 - loss: 0.3322 - val_accuracy: 0.9442 - val_loss: 0.3462\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.9314 - loss: 0.3785\n",
            "Test accuracy: 0.9417\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from tensorflow.keras.datasets import mnist\n",
        "from tensorflow.keras import layers, models, regularizers\n",
        "\n",
        "# Load the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "\n",
        "# Normalize the input data\n",
        "x_train = x_train.astype(\"float32\") / 255.0\n",
        "x_test = x_test.astype(\"float32\") / 255.0\n",
        "\n",
        "# üîπ FIX: Reshape 28x28 images into 784-length vectors\n",
        "x_train = x_train.reshape(-1, 784)\n",
        "x_test = x_test.reshape(-1, 784)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train = to_categorical(y_train, 10)\n",
        "y_test = to_categorical(y_test, 10)\n",
        "\n",
        "# Define a simple model with L2 regularization\n",
        "def build_l2_model():\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu',\n",
        "                     input_shape=(784,),\n",
        "                     kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.Dense(64, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(0.01)),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "model_l2 = build_l2_model()\n",
        "\n",
        "model_l2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model\n",
        "history = model_l2.fit(\n",
        "    x_train,\n",
        "    y_train,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate the model\n",
        "test_loss, test_acc = model_l2.evaluate(x_test, y_test)\n",
        "\n",
        "print(f\"Test accuracy: {test_acc:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.datasets import fashion_mnist\n",
        "def build_custom_l2_model(l2_strength=0.005):\n",
        "    model = models.Sequential([\n",
        "        layers.Dense(64, activation='relu',\n",
        "                     input_shape=(784,),\n",
        "                     kernel_regularizer=regularizers.l2(l2_strength)),\n",
        "        layers.Dense(64, activation='relu',\n",
        "                     kernel_regularizer=regularizers.l2(l2_strength)),\n",
        "        layers.Dense(10, activation='softmax')\n",
        "    ])\n",
        "    return model\n",
        "\n",
        "\n",
        "# Load the Fashion MNIST dataset\n",
        "(x_train_fashion, y_train_fashion), (x_test_fashion, y_test_fashion) = fashion_mnist.load_data()\n",
        "\n",
        "# Normalize the input data\n",
        "x_train_fashion = x_train_fashion.astype(\"float32\") / 255.0\n",
        "x_test_fashion = x_test_fashion.astype(\"float32\") / 255.0\n",
        "\n",
        "# Reshape 28x28 images into 784-length vectors\n",
        "x_train_fashion = x_train_fashion.reshape(-1, 784)\n",
        "x_test_fashion = x_test_fashion.reshape(-1, 784)\n",
        "\n",
        "# One-hot encode the labels\n",
        "y_train_fashion = to_categorical(y_train_fashion, 10)\n",
        "y_test_fashion = to_categorical(y_test_fashion, 10)\n",
        "\n",
        "\n",
        "model_l2_fashion = build_l2_model()\n",
        "\n",
        "model_l2_fashion.compile(\n",
        "    optimizer='SGD',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Training model on Fashion MNIST...\")\n",
        "history_fashion = model_l2_fashion.fit(\n",
        "    x_train_fashion,\n",
        "    y_train_fashion,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate the model on Fashion MNIST\n",
        "test_loss_fashion, test_acc_fashion = model_l2_fashion.evaluate(x_test_fashion, y_test_fashion)\n",
        "\n",
        "print(f\"Fashion MNIST Test accuracy: {test_acc_fashion:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nOZ3B5ZEETkt",
        "outputId": "7cec6482-065a-49bb-afcd-85a3be8aa23f"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on Fashion MNIST...\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6215 - loss: 2.7885 - val_accuracy: 0.7938 - val_loss: 1.7272\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8003 - loss: 1.5862 - val_accuracy: 0.8086 - val_loss: 1.2474\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8201 - loss: 1.1576 - val_accuracy: 0.8204 - val_loss: 0.9801\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8263 - loss: 0.9276 - val_accuracy: 0.8247 - val_loss: 0.8372\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8283 - loss: 0.8011 - val_accuracy: 0.8282 - val_loss: 0.7443\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8298 - loss: 0.7358 - val_accuracy: 0.8322 - val_loss: 0.6982\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8363 - loss: 0.6838 - val_accuracy: 0.8363 - val_loss: 0.6606\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8352 - loss: 0.6587 - val_accuracy: 0.8421 - val_loss: 0.6371\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.8397 - loss: 0.6375 - val_accuracy: 0.8294 - val_loss: 0.6381\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8443 - loss: 0.6203 - val_accuracy: 0.8386 - val_loss: 0.6217\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8323 - loss: 0.6384\n",
            "Fashion MNIST Test accuracy: 0.8276\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "c2f514d3",
        "outputId": "e3895afe-2d02-4108-bb7b-4f5bd43d6215"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Reshape the first image from the Fashion MNIST training set to its original 28x28 format\n",
        "first_fashion_image = x_train_fashion[0].reshape(28, 28)\n",
        "\n",
        "# Display the image\n",
        "plt.imshow(first_fashion_image, cmap='gray')\n",
        "plt.title('First Fashion MNIST Training Image')\n",
        "plt.axis('off') # Hide axes ticks and labels\n",
        "plt.show()\n",
        "\n",
        "print(f\"Shape of the first data point: {x_train_fashion[0].shape}\")\n",
        "print(f\"Pixel values of the first data point (flattened):\\n{x_train_fashion[0]}\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shape of the first data point: (784,)\n",
            "Pixel values of the first data point (flattened):\n",
            "[0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.00392157 0.         0.         0.05098039 0.28627452 0.\n",
            " 0.         0.00392157 0.01568628 0.         0.         0.\n",
            " 0.         0.00392157 0.00392157 0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.01176471 0.\n",
            " 0.14117648 0.53333336 0.49803922 0.24313726 0.21176471 0.\n",
            " 0.         0.         0.00392157 0.01176471 0.01568628 0.\n",
            " 0.         0.01176471 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.02352941 0.         0.4        0.8\n",
            " 0.6901961  0.5254902  0.5647059  0.48235294 0.09019608 0.\n",
            " 0.         0.         0.         0.04705882 0.03921569 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.60784316 0.9254902  0.8117647  0.69803923\n",
            " 0.41960785 0.6117647  0.6313726  0.42745098 0.2509804  0.09019608\n",
            " 0.3019608  0.50980395 0.28235295 0.05882353 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.00392157 0.         0.27058825\n",
            " 0.8117647  0.8745098  0.85490197 0.84705883 0.84705883 0.6392157\n",
            " 0.49803922 0.4745098  0.47843137 0.57254905 0.5529412  0.34509805\n",
            " 0.6745098  0.25882354 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157\n",
            " 0.00392157 0.00392157 0.         0.78431374 0.9098039  0.9098039\n",
            " 0.9137255  0.8980392  0.8745098  0.8745098  0.84313726 0.8352941\n",
            " 0.6431373  0.49803922 0.48235294 0.76862746 0.8980392  0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.7176471  0.88235295 0.84705883 0.8745098  0.89411765\n",
            " 0.92156863 0.8901961  0.8784314  0.87058824 0.8784314  0.8666667\n",
            " 0.8745098  0.9607843  0.6784314  0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.75686276\n",
            " 0.89411765 0.85490197 0.8352941  0.7764706  0.7058824  0.83137256\n",
            " 0.8235294  0.827451   0.8352941  0.8745098  0.8627451  0.9529412\n",
            " 0.7921569  0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.00392157\n",
            " 0.01176471 0.         0.04705882 0.85882354 0.8627451  0.83137256\n",
            " 0.85490197 0.7529412  0.6627451  0.8901961  0.8156863  0.85490197\n",
            " 0.8784314  0.83137256 0.8862745  0.77254903 0.81960785 0.20392157\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.02352941 0.\n",
            " 0.3882353  0.95686275 0.87058824 0.8627451  0.85490197 0.79607844\n",
            " 0.7764706  0.8666667  0.84313726 0.8352941  0.87058824 0.8627451\n",
            " 0.9607843  0.46666667 0.654902   0.21960784 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.01568628 0.         0.         0.21568628 0.9254902\n",
            " 0.89411765 0.9019608  0.89411765 0.9411765  0.9098039  0.8352941\n",
            " 0.85490197 0.8745098  0.91764706 0.8509804  0.8509804  0.81960785\n",
            " 0.36078432 0.         0.         0.         0.00392157 0.01568628\n",
            " 0.02352941 0.02745098 0.00784314 0.         0.         0.\n",
            " 0.         0.         0.92941177 0.8862745  0.8509804  0.8745098\n",
            " 0.87058824 0.85882354 0.87058824 0.8666667  0.84705883 0.8745098\n",
            " 0.8980392  0.84313726 0.85490197 1.         0.3019608  0.\n",
            " 0.         0.01176471 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.24313726 0.5686275  0.8\n",
            " 0.89411765 0.8117647  0.8352941  0.8666667  0.85490197 0.8156863\n",
            " 0.827451   0.85490197 0.8784314  0.8745098  0.85882354 0.84313726\n",
            " 0.8784314  0.95686275 0.62352943 0.         0.         0.\n",
            " 0.         0.         0.07058824 0.17254902 0.32156864 0.41960785\n",
            " 0.7411765  0.89411765 0.8627451  0.87058824 0.8509804  0.8862745\n",
            " 0.78431374 0.8039216  0.827451   0.9019608  0.8784314  0.91764706\n",
            " 0.6901961  0.7372549  0.98039216 0.972549   0.9137255  0.93333334\n",
            " 0.84313726 0.         0.         0.22352941 0.73333335 0.8156863\n",
            " 0.8784314  0.8666667  0.8784314  0.8156863  0.8        0.8392157\n",
            " 0.8156863  0.81960785 0.78431374 0.62352943 0.9607843  0.75686276\n",
            " 0.80784315 0.8745098  1.         1.         0.8666667  0.91764706\n",
            " 0.8666667  0.827451   0.8627451  0.9098039  0.9647059  0.\n",
            " 0.01176471 0.7921569  0.89411765 0.8784314  0.8666667  0.827451\n",
            " 0.827451   0.8392157  0.8039216  0.8039216  0.8039216  0.8627451\n",
            " 0.9411765  0.3137255  0.5882353  1.         0.8980392  0.8666667\n",
            " 0.7372549  0.6039216  0.7490196  0.8235294  0.8        0.81960785\n",
            " 0.87058824 0.89411765 0.88235295 0.         0.38431373 0.9137255\n",
            " 0.7764706  0.8235294  0.87058824 0.8980392  0.8980392  0.91764706\n",
            " 0.9764706  0.8627451  0.7607843  0.84313726 0.8509804  0.94509804\n",
            " 0.25490198 0.28627452 0.41568628 0.45882353 0.65882355 0.85882354\n",
            " 0.8666667  0.84313726 0.8509804  0.8745098  0.8745098  0.8784314\n",
            " 0.8980392  0.11372549 0.29411766 0.8        0.83137256 0.8\n",
            " 0.75686276 0.8039216  0.827451   0.88235295 0.84705883 0.7254902\n",
            " 0.77254903 0.80784315 0.7764706  0.8352941  0.9411765  0.7647059\n",
            " 0.8901961  0.9607843  0.9372549  0.8745098  0.85490197 0.83137256\n",
            " 0.81960785 0.87058824 0.8627451  0.8666667  0.9019608  0.2627451\n",
            " 0.1882353  0.79607844 0.7176471  0.7607843  0.8352941  0.77254903\n",
            " 0.7254902  0.74509805 0.7607843  0.7529412  0.7921569  0.8392157\n",
            " 0.85882354 0.8666667  0.8627451  0.9254902  0.88235295 0.84705883\n",
            " 0.78039217 0.80784315 0.7294118  0.70980394 0.69411767 0.6745098\n",
            " 0.70980394 0.8039216  0.80784315 0.4509804  0.         0.47843137\n",
            " 0.85882354 0.75686276 0.7019608  0.67058825 0.7176471  0.76862746\n",
            " 0.8        0.8235294  0.8352941  0.8117647  0.827451   0.8235294\n",
            " 0.78431374 0.76862746 0.7607843  0.7490196  0.7647059  0.7490196\n",
            " 0.7764706  0.7529412  0.6901961  0.6117647  0.654902   0.69411767\n",
            " 0.8235294  0.36078432 0.         0.         0.2901961  0.7411765\n",
            " 0.83137256 0.7490196  0.6862745  0.6745098  0.6862745  0.70980394\n",
            " 0.7254902  0.7372549  0.7411765  0.7372549  0.75686276 0.7764706\n",
            " 0.8        0.81960785 0.8235294  0.8235294  0.827451   0.7372549\n",
            " 0.7372549  0.7607843  0.7529412  0.84705883 0.6666667  0.\n",
            " 0.00784314 0.         0.         0.         0.25882354 0.78431374\n",
            " 0.87058824 0.92941177 0.9372549  0.9490196  0.9647059  0.9529412\n",
            " 0.95686275 0.8666667  0.8627451  0.75686276 0.7490196  0.7019608\n",
            " 0.7137255  0.7137255  0.70980394 0.6901961  0.6509804  0.65882355\n",
            " 0.3882353  0.22745098 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.15686275\n",
            " 0.23921569 0.17254902 0.28235295 0.16078432 0.13725491 0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_l2_fashion = build_l2_model()\n",
        "\n",
        "model_l2_fashion.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Training model on Fashion MNIST...\")\n",
        "history_fashion = model_l2_fashion.fit(\n",
        "    x_train_fashion,\n",
        "    y_train_fashion,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate the model on Fashion MNIST\n",
        "test_loss_fashion, test_acc_fashion = model_l2_fashion.evaluate(x_test_fashion, y_test_fashion)\n",
        "\n",
        "print(f\"Fashion MNIST Test accuracy: {test_acc_fashion:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1oBSQoupIp_G",
        "outputId": "5526f5e0-c920-40b1-cd55-9eb4194cfbd0"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on Fashion MNIST...\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7419 - loss: 1.4232 - val_accuracy: 0.8129 - val_loss: 0.7381\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8143 - loss: 0.7294 - val_accuracy: 0.8283 - val_loss: 0.6630\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.8188 - loss: 0.6740 - val_accuracy: 0.8294 - val_loss: 0.6373\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8232 - loss: 0.6381 - val_accuracy: 0.8338 - val_loss: 0.6027\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8269 - loss: 0.6145 - val_accuracy: 0.8433 - val_loss: 0.5838\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8336 - loss: 0.5931 - val_accuracy: 0.8273 - val_loss: 0.6088\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8337 - loss: 0.5865 - val_accuracy: 0.8383 - val_loss: 0.5837\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8336 - loss: 0.5835 - val_accuracy: 0.8063 - val_loss: 0.6383\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8333 - loss: 0.5760 - val_accuracy: 0.8238 - val_loss: 0.5838\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8347 - loss: 0.5657 - val_accuracy: 0.8444 - val_loss: 0.5477\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8432 - loss: 0.5624\n",
            "Fashion MNIST Test accuracy: 0.8361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "64031240"
      },
      "source": [
        "### What is Data Augmentation?\n",
        "\n",
        "Data augmentation artificially expands the training dataset by creating modified versions of existing images. This helps expose the model to a wider variety of data, making it more robust and less prone to overfitting.\n",
        "\n",
        "Common augmentation techniques for images include:\n",
        "*   **Rotation:** Rotating images by a certain degree.\n",
        "*   **Width/Height Shift:** Shifting images horizontally or vertically.\n",
        "*   **Zoom:** Randomly zooming in or out of images.\n",
        "*   **Horizontal/Vertical Flip:** Flipping images horizontally or vertically.\n",
        "*   **Brightness Adjustment:** Randomly changing the brightness of images.\n",
        "\n",
        "We'll use `tf.keras.preprocessing.image.ImageDataGenerator` to apply these transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1584fd89",
        "outputId": "9b4588f6-f815-4180-ee12-2431137bea5a"
      },
      "source": [
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "import numpy as np\n",
        "\n",
        "# Reshape x_train_fashion back to 28x28x1 for ImageDataGenerator\n",
        "x_train_fashion_reshaped = x_train_fashion.reshape(-1, 28, 28, 1)\n",
        "\n",
        "# Define the data augmentation generator\n",
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=10,        # Rotate images by a maximum of 10 degrees\n",
        "    width_shift_range=0.1,    # Shift images horizontally by up to 10% of the width\n",
        "    height_shift_range=0.1,   # Shift images vertically by up to 10% of the height\n",
        "    zoom_range=0.1,           # Zoom in/out by up to 10%\n",
        "    horizontal_flip=True,     # Randomly flip images horizontally\n",
        "    fill_mode='nearest'       # Fill in new pixels created by transformations\n",
        ")\n",
        "\n",
        "# Fit the generator to the training data\n",
        "datagen.fit(x_train_fashion_reshaped)\n",
        "\n",
        "print(\"ImageDataGenerator configured and fitted to training data.\")"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ImageDataGenerator configured and fitted to training data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4859989e"
      },
      "source": [
        "Let's visualize an original image and a few augmented versions of it to see the effects of the transformations."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 266
        },
        "id": "9a5bc0ec",
        "outputId": "6f5018fd-9ba3-4265-a119-d6b9aa6b38b2"
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Take a sample image (e.g., the first image from the reshaped training set)\n",
        "sample_image = x_train_fashion_reshaped[0]\n",
        "\n",
        "# Generate a batch of augmented images\n",
        "augmented_images_iterator = datagen.flow(np.expand_dims(sample_image, 0), batch_size=1)\n",
        "\n",
        "plt.figure(figsize=(10, 5))\n",
        "\n",
        "# Original image\n",
        "plt.subplot(1, 4, 1)\n",
        "plt.imshow(sample_image.reshape(28, 28), cmap='gray')\n",
        "plt.title('Original Image')\n",
        "plt.axis('off')\n",
        "\n",
        "# Display 3 augmented versions\n",
        "for i in range(3):\n",
        "    plt.subplot(1, 4, i + 2)\n",
        "    augmented_image = next(augmented_images_iterator)[0].reshape(28, 28)\n",
        "    plt.imshow(augmented_image, cmap='gray')\n",
        "    plt.title(f'Augmented {i+1}')\n",
        "    plt.axis('off')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"Displayed original and augmented images.\")"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x500 with 4 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Displayed original and augmented images.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_l2_fashion = build_l2_model()\n",
        "\n",
        "model_l2_fashion.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "print(\"Training model on Fashion MNIST...\")\n",
        "history_fashion = model_l2_fashion.fit(\n",
        "    x_train_fashion,\n",
        "    y_train_fashion,\n",
        "    epochs=10,\n",
        "    validation_split=0.2,\n",
        "    batch_size=32\n",
        ")\n",
        "\n",
        "# Evaluate the model on Fashion MNIST\n",
        "test_loss_fashion, test_acc_fashion = model_l2_fashion.evaluate(x_test_fashion, y_test_fashion)\n",
        "\n",
        "print(f\"Fashion MNIST Test accuracy: {test_acc_fashion:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "09tt3i5gLGDe",
        "outputId": "a541653b-8e1e-4e22-b217-5a3abcf610c2"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model on Fashion MNIST...\n",
            "Epoch 1/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7371 - loss: 1.4176 - val_accuracy: 0.8100 - val_loss: 0.7628\n",
            "Epoch 2/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8137 - loss: 0.7224 - val_accuracy: 0.8222 - val_loss: 0.6654\n",
            "Epoch 3/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8215 - loss: 0.6615 - val_accuracy: 0.8248 - val_loss: 0.6361\n",
            "Epoch 4/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8289 - loss: 0.6332 - val_accuracy: 0.8353 - val_loss: 0.6029\n",
            "Epoch 5/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8272 - loss: 0.6182 - val_accuracy: 0.8356 - val_loss: 0.5937\n",
            "Epoch 6/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8301 - loss: 0.6040 - val_accuracy: 0.8352 - val_loss: 0.5685\n",
            "Epoch 7/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8362 - loss: 0.5833 - val_accuracy: 0.8260 - val_loss: 0.5953\n",
            "Epoch 8/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8327 - loss: 0.5847 - val_accuracy: 0.8317 - val_loss: 0.5772\n",
            "Epoch 9/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8386 - loss: 0.5634 - val_accuracy: 0.8378 - val_loss: 0.5604\n",
            "Epoch 10/10\n",
            "\u001b[1m1500/1500\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8386 - loss: 0.5608 - val_accuracy: 0.8329 - val_loss: 0.5640\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8364 - loss: 0.5738\n",
            "Fashion MNIST Test accuracy: 0.8307\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"Training model with data augmentation on Fashion MNIST...\")\n",
        "\n",
        "# Build a fresh model to train with augmentation (optional, but good practice)\n",
        "# The original build_l2_model expects flattened input, but datagen.flow outputs 28x28x1 images.\n",
        "# We need to adapt the model to handle 28x28x1 input by adding a Flatten layer.\n",
        "model_l2_fashion_augmented = models.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28, 1)), # Flatten the 28x28x1 images to 784 features\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.01)),\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "model_l2_fashion_augmented.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# Train the model using the data generator\n",
        "history_fashion_augmented = model_l2_fashion_augmented.fit(\n",
        "    datagen.flow(x_train_fashion_reshaped, y_train_fashion, batch_size=32),\n",
        "    epochs=10,\n",
        "    validation_data=(x_test_fashion.reshape(-1, 28, 28, 1), y_test_fashion), # Validate on unaugmented test data\n",
        "    steps_per_epoch=len(x_train_fashion_reshaped) // 32\n",
        ")\n",
        "\n",
        "# Evaluate the model on Fashion MNIST (unaugmented test set)\n",
        "test_loss_fashion_augmented, test_acc_fashion_augmented = model_l2_fashion_augmented.evaluate(\n",
        "    x_test_fashion.reshape(-1, 28, 28, 1),\n",
        "    y_test_fashion\n",
        ")\n",
        "\n",
        "print(f\"Fashion MNIST Test accuracy with augmentation: {test_acc_fashion_augmented:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-tiLNvCtMDoI",
        "outputId": "cc662516-5de0-401c-cc79-4fe2e52d5bf4"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with data augmentation on Fashion MNIST...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 15ms/step - accuracy: 0.6075 - loss: 1.6583 - val_accuracy: 0.7222 - val_loss: 0.9338\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.6906 - loss: 1.0045 - val_accuracy: 0.6911 - val_loss: 0.9008\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.6987 - loss: 0.9466 - val_accuracy: 0.7485 - val_loss: 0.8213\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 15ms/step - accuracy: 0.7048 - loss: 0.9199 - val_accuracy: 0.7415 - val_loss: 0.8042\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7065 - loss: 0.9084 - val_accuracy: 0.7377 - val_loss: 0.8125\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7123 - loss: 0.8809 - val_accuracy: 0.7401 - val_loss: 0.8042\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7167 - loss: 0.8689 - val_accuracy: 0.7574 - val_loss: 0.7614\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7149 - loss: 0.8644 - val_accuracy: 0.7550 - val_loss: 0.7558\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7193 - loss: 0.8580 - val_accuracy: 0.7406 - val_loss: 0.7900\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 15ms/step - accuracy: 0.7206 - loss: 0.8440 - val_accuracy: 0.7503 - val_loss: 0.7595\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7506 - loss: 0.7537\n",
            "Fashion MNIST Test accuracy with augmentation: 0.7503\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e879100",
        "outputId": "fe56d441-41ad-41d4-8b7e-a4a643cb346b"
      },
      "source": [
        "print(\"Training model with data augmentation and reduced L2 regularization on Fashion MNIST...\")\n",
        "\n",
        "# 1. Define a new sequential model named model_l2_fashion_augmented_reduced_l2\n",
        "model_l2_fashion_augmented_reduced_l2 = models.Sequential([\n",
        "    layers.Flatten(input_shape=(28, 28, 1)), # Flatten the 28x28x1 images to 784 features\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)), # Reduced L2 strength\n",
        "    layers.Dense(64, activation='relu',\n",
        "                 kernel_regularizer=regularizers.l2(0.001)), # Reduced L2 strength\n",
        "    layers.Dense(10, activation='softmax')\n",
        "])\n",
        "\n",
        "# 2. Compile the model\n",
        "model_l2_fashion_augmented_reduced_l2.compile(\n",
        "    optimizer='adam',\n",
        "    loss='categorical_crossentropy',\n",
        "    metrics=['accuracy']\n",
        ")\n",
        "\n",
        "# 3. Train the model using the data generator\n",
        "history_fashion_augmented_reduced_l2 = model_l2_fashion_augmented_reduced_l2.fit(\n",
        "    datagen.flow(x_train_fashion_reshaped, y_train_fashion, batch_size=32),\n",
        "    epochs=10,\n",
        "    validation_data=(x_test_fashion.reshape(-1, 28, 28, 1), y_test_fashion), # Validate on unaugmented test data\n",
        "    steps_per_epoch=len(x_train_fashion_reshaped) // 32\n",
        ")\n",
        "\n",
        "# 4. Evaluate the trained model on Fashion MNIST (unaugmented test set)\n",
        "test_loss_fashion_augmented_reduced_l2, test_acc_fashion_augmented_reduced_l2 = model_l2_fashion_augmented_reduced_l2.evaluate(\n",
        "    x_test_fashion.reshape(-1, 28, 28, 1),\n",
        "    y_test_fashion\n",
        ")\n",
        "\n",
        "print(f\"Fashion MNIST Test accuracy with augmentation and reduced L2: {test_acc_fashion_augmented_reduced_l2:.4f}\")"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model with data augmentation and reduced L2 regularization on Fashion MNIST...\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/keras/src/trainers/data_adapters/py_dataset_adapter.py:121: UserWarning: Your `PyDataset` class should call `super().__init__(**kwargs)` in its constructor. `**kwargs` can include `workers`, `use_multiprocessing`, `max_queue_size`. Do not pass these arguments to `fit()`, as they will be ignored.\n",
            "  self._warn_if_super_not_called()\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m30s\u001b[0m 15ms/step - accuracy: 0.6202 - loss: 1.1708 - val_accuracy: 0.7520 - val_loss: 0.7516\n",
            "Epoch 2/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7316 - loss: 0.8231 - val_accuracy: 0.7559 - val_loss: 0.7273\n",
            "Epoch 3/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 15ms/step - accuracy: 0.7441 - loss: 0.7763 - val_accuracy: 0.7792 - val_loss: 0.6891\n",
            "Epoch 4/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 15ms/step - accuracy: 0.7552 - loss: 0.7421 - val_accuracy: 0.7601 - val_loss: 0.6955\n",
            "Epoch 5/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 15ms/step - accuracy: 0.7579 - loss: 0.7332 - val_accuracy: 0.7854 - val_loss: 0.6822\n",
            "Epoch 6/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7620 - loss: 0.7167 - val_accuracy: 0.7953 - val_loss: 0.6431\n",
            "Epoch 7/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7648 - loss: 0.7051 - val_accuracy: 0.7964 - val_loss: 0.6277\n",
            "Epoch 8/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m42s\u001b[0m 15ms/step - accuracy: 0.7655 - loss: 0.6983 - val_accuracy: 0.7969 - val_loss: 0.6256\n",
            "Epoch 9/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7695 - loss: 0.7009 - val_accuracy: 0.7879 - val_loss: 0.6192\n",
            "Epoch 10/10\n",
            "\u001b[1m1875/1875\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 15ms/step - accuracy: 0.7698 - loss: 0.6883 - val_accuracy: 0.8103 - val_loss: 0.5904\n",
            "\u001b[1m313/313\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.8144 - loss: 0.5853\n",
            "Fashion MNIST Test accuracy with augmentation and reduced L2: 0.8103\n"
          ]
        }
      ]
    }
  ]
}